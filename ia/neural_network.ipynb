{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = df.shape\n",
    "print(f'Rows and columns in one JSON file is {df_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 First Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.head(10)\n",
    "print(f'First 10 columns in one JSON file is {df_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Columns where Column Name Like \"nam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.filter(regex='nam').columns\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset General Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", df.shape[0] , \" and \", df.shape[1])\n",
    "print(\"The years in this dataset are: \", df.year.unique())\n",
    "print(\"The artists covered in this dataset are: \", list(df.artist.unique()))\n",
    "print(\"The genders covered are: \", list(df.gender.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Dataset Rows Group by \"gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Count':df.gender.value_counts()})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Dataset Rows Ordered, Group by \"gender\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=['gender'],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"bought\" renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'bought':'is_bought'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of \"null\" values of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.likes.isnull()]\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'is_bought', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cathegorical Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = df.copy()\n",
    "data = df[['is_bought', 'likes','name', 'artist', 'year', 'gender']]\n",
    "categorical_columns  = ['name', 'artist', 'year','gender']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')\n",
    "    \n",
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "\n",
    "print(f'The column types are :')\n",
    "print('#########')\n",
    "for col in data.dtypes:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cathegorical Column Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(data[categorical_columns], drop_first=True)\n",
    "data_dummies = data_dummies.replace({True: 1, False: 0})\n",
    "not_categorical_columns  = ['is_bought','likes']\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Dataset General Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", df.shape[0] , \" and \", df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 First Normalized Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.head(10)\n",
    "print(f'First 10 columns in one JSON file is {df_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Dataset Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"bought\" renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'is_bought':'target'}, inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['likes']\n",
    "\n",
    "X = data[features]\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_of_epochs = 1000\n",
    "learning_rate=0.01\n",
    "weight_decay=0.0001\n",
    "test_size = 0.33\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self)-> None:\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def __flush__(self) -> None:\n",
    "        self.writer.flush\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray) -> None:  \n",
    "    self.X = torch.from_numpy(X.astype(np.float32))\n",
    "    self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    self.len = self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self) -> int:\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Batch Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, train:Data, test:Data) -> None:\n",
    "        self.train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        self.test = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Batch Loader Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = Data(X_train.values, y_train.values)\n",
    "testdata = Data(X_test.values, y_test.values)\n",
    "\n",
    "# create Loader to read the data within batch sizes and put into memory. \n",
    "loader = Loader(traindata, testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module): # all the dependencies from torch will be given to this class [parent class] # nn.Module contains all the building block of neural networks:\n",
    "  def __init__(self,input_dim):\n",
    "    super(LinearRegression,self).__init__()   # building connection with parent and child classes\n",
    "    self.fc1=nn.Linear(input_dim,10)          # hidden layer 1\n",
    "    self.fc2=nn.Linear(10,5)                  # hidden layer 2\n",
    "    self.fc3=nn.Linear(5,3)                   # hidden layer 3\n",
    "    self.fc4=nn.Linear(3,1)                   # last layer\n",
    "\n",
    "  def forward(self,d):\n",
    "    out=torch.relu(self.fc1(d))              # input * weights + bias for layer 1\n",
    "    out=torch.relu(self.fc2(out))            # input * weights + bias for layer 2\n",
    "    out=torch.relu(self.fc3(out))            # input * weights + bias for layer 3\n",
    "    out=self.fc4(out)                        # input * weights + bias for last layer\n",
    "    return out                               # final outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "torch.manual_seed(42)  # to make initilized weights stable:\n",
    "model = LinearRegression(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function, Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function with Mean Squared Error loss and an optimizer with Adam optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizers=optim.Adam(params=model.parameters(),lr=learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload()-> None:\n",
    "    # create a dummy input tuple  \n",
    "    dummy_input = (1)\n",
    "\n",
    "    # load the saved model and do the inference again \n",
    "    load_model=LinearRegression(dummy_input)\n",
    "    load_model.load_state_dict(torch.load('saved/Network.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Parameters Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save() -> None:\n",
    "    filename=Path('saved')\n",
    "    filename.mkdir(parents=True,exist_ok=True)    \n",
    "    model_name='Network.pth' \n",
    "\n",
    "    saving_path=filename/model_name   \n",
    "    torch.save(obj=model.state_dict(),f=saving_path)\n",
    "\n",
    "    print(\" \") \n",
    "    print('Model has been converted to PTH') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(): \n",
    "\n",
    "    # set the model to inference mode \n",
    "    model.eval() \n",
    "\n",
    "    # create a dummy input tensor  \n",
    "    dummy_input = torch.randn(1)  \n",
    "\n",
    "    # export the model   \n",
    "    torch.onnx.export(model,        # model being run \n",
    "         dummy_input,               # model input (or a tuple for multiple inputs) \n",
    "         \"saved/Network.onnx\",      # where to save the model  \n",
    "         export_params=True,        # store the trained parameter weights inside the model file \n",
    "         opset_version=11,          # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['input'],   # the model's input names \n",
    "         output_names = ['output'], # the model's output names \n",
    "         dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'output' : {0 : 'batch_size'}}) \n",
    "    print(\" \") \n",
    "    print('Model has been converted to ONNX') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect() -> None:\n",
    "    \n",
    "    # create a dummy input tensor  \n",
    "    dummy_input = torch.randn(1)  \n",
    "\n",
    "    # inspect the model using TensorBoard\n",
    "    logger.writer.add_graph(model, dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_of_epochs: float = 1000) -> None:\n",
    "    \n",
    "    # enable model evaluation\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0  \n",
    "    best_accuracy = 0.0\n",
    " \n",
    "    # define execution device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logger.writer.add_text('config/model', model.__class__.__name__)\n",
    "    logger.writer.add_text('config/optimizer', optimizers.__class__.__name__)\n",
    "    logger.writer.add_text('config/device', str(device)) \n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(num_of_epochs):\n",
    "\n",
    "        batch_loss = 0.0           \n",
    "        \n",
    "        # disable gradient computation\n",
    "        with torch.no_grad():\n",
    "            for b, data in enumerate(loader.test, 0):        \n",
    "            \n",
    "                # get the inputs\n",
    "                inputs, targets = data            \n",
    "\n",
    "                # run the model on the test set to predict labels                \n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss_value = loss(outputs, targets)\n",
    "              \n",
    "                batch_loss += loss_value.item()     \n",
    "\n",
    "                if b == data.__len__():         \n",
    "\n",
    "                    # log epoch loss\n",
    "                    logger.writer.add_scalar('test/batch/loss',batch_loss / data.__len__(),epoch * len(loader.train) + b)             \n",
    "\n",
    "                    # zero the loss\n",
    "                    batch_loss = 0.0      \n",
    "\n",
    "                global_step = epoch * len(loader.test) + b\n",
    "                \n",
    "\n",
    "                # log the batch loss\n",
    "                logger.writer.add_scalar('test/loss', loss_value.item(), global_step=global_step)\n",
    "                logger.writer.add_scalar('test/confidence', torch.mean(torch.max(torch.softmax(outputs, dim=1), dim=1)[0]).item(), global_step=global_step)\n",
    "\n",
    "                logger.writer.add_histogram('test/outputs', torch.max(outputs, dim=1)[0], global_step=global_step)\n",
    "                logger.writer.add_histogram('test/confidences', torch.max(torch.softmax(outputs, dim=1), dim=1)[0], global_step=global_step)\n",
    "                \n",
    "                \n",
    "                # the label with the highest energy will be our prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                accuracy += (predicted == targets).sum().item()\n",
    "        \n",
    "    # compute the accuracy over all test records\n",
    "    accuracy = (100 * accuracy / total)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "      save()     \n",
    "      best_accuracy = accuracy \n",
    "      \n",
    "    logger.__flush__()\n",
    "\n",
    "    print(\" \")\n",
    "    print('Model has been tested')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_of_epochs: float = 1000) -> None:\n",
    " \n",
    "  # define execution device\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  logger.writer.add_text('config/model', model.__class__.__name__)\n",
    "  logger.writer.add_text('config/optimizer', optimizers.__class__.__name__)\n",
    "  logger.writer.add_text('config/device', str(device)) \n",
    " \n",
    "  model.to(device)\n",
    "\n",
    "  # loop over the dataset multiple times\n",
    "  for epoch in range(num_of_epochs):\n",
    "\n",
    "    batch_loss = 0.0     \n",
    "   \n",
    "    for b, data in enumerate(loader.train, 0):  \n",
    "\n",
    "      # get the inputs\n",
    "      inputs, targets = data      \n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizers.zero_grad()\n",
    "\n",
    "      # predict classes using records from the training set\n",
    "      outputs=model(inputs) \n",
    "\n",
    "      # compute the loss based on model output and real targets\n",
    "      loss_value=loss(outputs, targets)    \n",
    "     \n",
    "      # backpropagate the loss\n",
    "      loss_value.backward()  \n",
    "\n",
    "      # adjust parameters based on the calculated gradients\n",
    "      optimizers.step() \n",
    "\n",
    "      global_step = epoch * len(loader.train) + b\n",
    "\n",
    "      # log the batch loss\n",
    "      logger.writer.add_scalar('train/loss', loss_value.item(), global_step=global_step)\n",
    "      logger.writer.add_scalar('train/confidence', torch.mean(torch.max(torch.softmax(outputs, dim=1), dim=1)[0]).item(), global_step=global_step)\n",
    "\n",
    "      logger.writer.add_histogram('train/outputs', torch.max(outputs, dim=1)[0], global_step=global_step)\n",
    "      logger.writer.add_histogram('train/confidences', torch.max(torch.softmax(outputs, dim=1), dim=1)[0], global_step=global_step)\n",
    "     \n",
    "      batch_loss += loss_value.item()     \n",
    "     \n",
    "      if b == data.__len__():         \n",
    "\n",
    "        # log the epoch loss\n",
    "        logger.writer.add_scalar('train/batch/loss',batch_loss / data.__len__(),epoch * len(loader.train) + b)             \n",
    "\n",
    "        # zero the loss\n",
    "        batch_loss = 0.0   \n",
    "\n",
    "  logger.__flush__()\n",
    "\n",
    "print(\" \")\n",
    "print('Model has been trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del logger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
