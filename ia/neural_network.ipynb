{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.onnx \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = df.shape\n",
    "print(f'Rows and columns in one JSON file is {df_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.head(10)\n",
    "print(f'First 10 columns in one JSON file is {df_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.filter(regex='nam').columns\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", df.shape[0] , \" and \", df.shape[1])\n",
    "print(\"The years in this dataset are: \", df.year.unique())\n",
    "print(\"The artists covered in this dataset are: \", list(df.artist.unique()))\n",
    "print(\"The genders covered are: \", list(df.gender.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Count':df.gender.value_counts()})\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=['gender'],ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'bought':'is_bought'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.likes.isnull()]\n",
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'is_bought', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = df.copy()\n",
    "data = df[['is_bought', 'likes','name', 'artist', 'year', 'gender']]\n",
    "categorical_columns  = ['name', 'artist', 'year','gender']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')\n",
    "    \n",
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in data.columns:\n",
    "    print(col)\n",
    "\n",
    "print(f'The column types are :')\n",
    "print('#########')\n",
    "for col in data.dtypes:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(data[categorical_columns], drop_first=True)\n",
    "data_dummies = data_dummies.replace({True: 1, False: 0})\n",
    "not_categorical_columns  = ['is_bought','likes']\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", data.shape[0] , \" and \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'is_bought':'target'}, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_of_epochs = 1000\n",
    "learning_rate=0.01\n",
    "weight_decay=0.0001\n",
    "test_size = 0.33\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self)-> None:\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "    def __flush__(self) -> None:\n",
    "        self.writer.flush\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        self.writer.flush()\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray) -> None:  \n",
    "    self.X = torch.from_numpy(X.astype(np.float32))\n",
    "    self.y = torch.from_numpy(y.astype(np.float32))\n",
    "    self.len = self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, index: int) -> tuple:\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self) -> int:\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, train:Data, test:Data) -> None:\n",
    "        self.train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        self.test = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = Data(X_train.values, y_train.values)\n",
    "testdata = Data(X_test.values, y_test.values)\n",
    "\n",
    "# create Loader to read the data within batch sizes and put into memory. \n",
    "loader = Loader(traindata, testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module): # all the dependencies from torch will be given to this class [parent class] # nn.Module contains all the building block of neural networks:\n",
    "  def __init__(self,input_dim):\n",
    "    super(LinearRegression,self).__init__()   # building connection with parent and child classes\n",
    "    self.fc1=nn.Linear(input_dim,10)          # hidden layer 1\n",
    "    self.fc2=nn.Linear(10,5)                  # hidden layer 2\n",
    "    self.fc3=nn.Linear(5,3)                   # hidden layer 3\n",
    "    self.fc4=nn.Linear(3,1)                   # last layer\n",
    "\n",
    "  def forward(self,d):\n",
    "    out=torch.relu(self.fc1(d))              # input * weights + bias for layer 1\n",
    "    out=torch.relu(self.fc2(out))            # input * weights + bias for layer 2\n",
    "    out=torch.relu(self.fc3(out))            # input * weights + bias for layer 3\n",
    "    out=self.fc4(out)                        # input * weights + bias for last layer\n",
    "    return out                               # final outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "torch.manual_seed(42)  # to make initilized weights stable:\n",
    "model = LinearRegression(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function with Mean Squared Error loss and an optimizer with Adam optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizers=optim.Adam(params=model.parameters(),lr=learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load()-> None:\n",
    "    # let's create a dummy input tuple  \n",
    "    dummy_input = (1)\n",
    "\n",
    "    # we can load the saved model and do the inference again \n",
    "    load_model=LinearRegression(dummy_input)\n",
    "    load_model.load_state_dict(torch.load('saved/Network.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model \n",
    "def save() -> None:\n",
    "    filename=Path('saved')\n",
    "    filename.mkdir(parents=True,exist_ok=True)    \n",
    "    model_name='Network.pth' \n",
    "\n",
    "    saving_path=filename/model_name   \n",
    "    torch.save(obj=model.state_dict(),f=saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Convert to ONNX \n",
    "def export(): \n",
    "\n",
    "    # set the model to inference mode \n",
    "    model.eval() \n",
    "\n",
    "    # let's create a dummy input tensor  \n",
    "    dummy_input = torch.randn(1)  \n",
    "\n",
    "    # export the model   \n",
    "    torch.onnx.export(model,         # model being run \n",
    "         dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "         \"saved/Network.onnx\",       # where to save the model  \n",
    "         export_params=True,  # store the trained parameter weights inside the model file \n",
    "         opset_version=11,    # the ONNX version to export the model to \n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "         input_names = ['input'],   # the model's input names \n",
    "         output_names = ['output'], # the model's output names \n",
    "         dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes \n",
    "                                'output' : {0 : 'batch_size'}}) \n",
    "    print(\" \") \n",
    "    print('Model has been converted to ONNX') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Inspect the model on TensorBoard \n",
    "def inspect() -> None:\n",
    "    \n",
    "    # let's create a dummy input tensor  \n",
    "    dummy_input = torch.randn(1)  \n",
    "\n",
    "    # we can inspect the model using TensorBoard\n",
    "    logger.writer.add_graph(model, dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model with the test dataset and print the accuracy for the test records\n",
    "def test() -> None:\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader.test:\n",
    "            inputs, targets = data\n",
    "            # run the model on the test set to predict labels\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            accuracy += (predicted == targets).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test records\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function \n",
    "def train(num_of_epochs: float = 1000) -> None:\n",
    "\n",
    "  best_accuracy = 0.0\n",
    "\n",
    "  # define your execution device\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  \n",
    "  print(\"The model will be running on\", device, \"device\")\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  # loop over the dataset multiple times\n",
    "  for epoch in range(num_of_epochs):\n",
    "\n",
    "    running_loss = 0.0     \n",
    "   \n",
    "    for i, data in enumerate(loader.train, 0):  \n",
    "\n",
    "      # get the inputs\n",
    "      inputs, targets = data\n",
    "      inputs, targets = inputs.float(), targets.float()\n",
    "      targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizers.zero_grad()\n",
    "\n",
    "      # predict classes using records from the training set\n",
    "      outputs=model(inputs) \n",
    "\n",
    "      # compute the loss based on model output and real targets\n",
    "      loss_value=loss(outputs, targets)    \n",
    "     \n",
    "      # backpropagate the loss\n",
    "      loss_value.backward()  \n",
    "\n",
    "      # adjust parameters based on the calculated gradients\n",
    "      optimizers.step() \n",
    "\n",
    "      # let's print statistics for every batch\n",
    "      running_loss += loss_value.item()     \n",
    "     \n",
    "      if i == data.__len__():         \n",
    "\n",
    "        # log the running loss\n",
    "        logger.writer.add_scalar('training loss',running_loss / data.__len__(),epoch * len(loader.train) + i)      \n",
    "          \n",
    "       # print once per epoch\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch, i , running_loss / num_of_epochs))\n",
    "\n",
    "        # zero the loss\n",
    "        running_loss = 0.0      \n",
    "     \n",
    "\n",
    "    # compute and print the average accuracy fo this epoch when tested over all test records\n",
    "    accuracy = test()\n",
    "    print('For epoch', epoch,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "      \n",
    "    # we want to save the model if the accuracy is the best\n",
    "    if accuracy > best_accuracy:\n",
    "      save()     \n",
    "      best_accuracy = accuracy  \n",
    "\n",
    "  logger.__flush__()\n",
    "\n",
    "  print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del logger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
